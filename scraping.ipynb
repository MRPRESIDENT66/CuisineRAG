{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Scraping Component\n",
    "This notebook automatically discovers and scrapes all South Asian cuisine related pages\n",
    "from Wikipedia and saves them as a JSON corpus for use in the CuisineRAG system.\n",
    "\n",
    "**Output:** `data/raw/south_asian_corpus.json`"
   ],
   "id": "title_md"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Install & Import Libraries",
   "id": "install_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import wikipediaapi\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Initialize Wikipedia API (replace with your real student email)\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    user_agent='CuisineRAG/1.0 (replace-with-your-email@domain.com)'\n",
    ")\n",
    "\n",
    "print('Libraries loaded successfully!')\n"
   ],
   "id": "import_code"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Discover All South Asian Food Pages\n",
    "We start from the main South Asian cuisine Wikipedia page and extract\n",
    "all linked dish/food pages automatically."
   ],
   "id": "discover_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Entry point pages for South Asian cuisine\n",
    "SEED_PAGES = [\n",
    "    'South_Asian_cuisine',\n",
    "    'Indian_cuisine',\n",
    "    'Pakistani_cuisine',\n",
    "    'Bangladeshi_cuisine',\n",
    "    'Sri_Lankan_cuisine',\n",
    "    'Nepalese_cuisine',\n",
    "]\n",
    "\n",
    "EXCLUDE_TITLE_KEYWORDS = {\n",
    "    'template', 'list of', 'index of', 'disambiguation', 'category',\n",
    "    'outline of', 'timeline of', 'history of', 'geography of', 'demographics'\n",
    "}\n",
    "\n",
    "FOOD_KEYWORDS = [\n",
    "    'cuisine', 'food', 'dish', 'recipe', 'cooking', 'curry', 'rice', 'bread',\n",
    "    'spice', 'biryani', 'masala', 'dal', 'roti', 'naan', 'chutney', 'kebab',\n",
    "    'samosa', 'dessert', 'sweet', 'snack', 'drink', 'beverage', 'soup',\n",
    "    'salad', 'chicken', 'lamb', 'mutton', 'lentil', 'vegetable', 'paneer'\n",
    "]\n",
    "\n",
    "\n",
    "def normalize_page_title(title: str) -> str:\n",
    "    \"\"\"Normalize wikipedia page keys into comparable title strings.\"\"\"\n",
    "    cleaned = unquote(title).replace('/wiki/', '').replace('_', ' ').strip()\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def is_title_candidate(page_title: str) -> bool:\n",
    "    \"\"\"Title-level filter to remove obvious non-food/non-article pages.\"\"\"\n",
    "    title = normalize_page_title(page_title).lower()\n",
    "    if ':' in title:\n",
    "        return False\n",
    "    if any(bad in title for bad in EXCLUDE_TITLE_KEYWORDS):\n",
    "        return False\n",
    "    return any(k in title for k in FOOD_KEYWORDS)\n",
    "\n",
    "\n",
    "def discover_links_from_seed(seed_title: str) -> set[str]:\n",
    "    \"\"\"Discover linked pages from a seed article via wikipediaapi links.\"\"\"\n",
    "    page = wiki.page(seed_title)\n",
    "    if not page.exists():\n",
    "        print(f'  ! Seed missing: {seed_title}')\n",
    "        return set()\n",
    "\n",
    "    discovered = set()\n",
    "    for linked_title in page.links.keys():\n",
    "        if is_title_candidate(linked_title):\n",
    "            discovered.add(linked_title)\n",
    "    return discovered\n",
    "\n",
    "\n",
    "print('Discovering pages from seed pages...')\n",
    "all_candidate_pages = set()\n",
    "discovery_log = []\n",
    "\n",
    "for seed in SEED_PAGES:\n",
    "    print(f'  Scanning: {seed}')\n",
    "    links = discover_links_from_seed(seed)\n",
    "    all_candidate_pages.update(links)\n",
    "    discovery_log.append({'seed': seed, 'discovered_candidates': len(links)})\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f'\\nTotal candidate pages found: {len(all_candidate_pages)}')\n"
   ],
   "id": "discover_code"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Filter Relevant Pages\n",
    "Not all links are food related. We filter by checking if the page\n",
    "title or content contains food/cuisine related keywords."
   ],
   "id": "filter_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape_wikipedia_page(page_title: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Scrape a Wikipedia page and return structured data.\n",
    "    Returns None for missing pages.\n",
    "    \"\"\"\n",
    "    page = wiki.page(page_title)\n",
    "    if not page.exists():\n",
    "        return None\n",
    "\n",
    "    text = f'# {page.title}\\n\\n'\n",
    "    text += (page.summary or '') + '\\n\\n'\n",
    "\n",
    "    for section in page.sections:\n",
    "        text += f'## {section.title}\\n'\n",
    "        text += (section.text or '') + '\\n\\n'\n",
    "        for subsection in section.sections:\n",
    "            text += f'### {subsection.title}\\n'\n",
    "            text += (subsection.text or '') + '\\n\\n'\n",
    "\n",
    "    return {\n",
    "        'title': page.title,\n",
    "        'source_title': page_title,\n",
    "        'url': page.fullurl,\n",
    "        'summary': page.summary,\n",
    "        'text': text.strip(),\n",
    "        'num_characters': len(text),\n",
    "    }\n",
    "\n",
    "\n",
    "def content_food_density(text: str) -> float:\n",
    "    \"\"\"Rough ratio of food-keyword hits in text; used as content-level filter.\"\"\"\n",
    "    lowered = text.lower()\n",
    "    hits = sum(lowered.count(k) for k in FOOD_KEYWORDS)\n",
    "    token_count = max(1, len(lowered.split()))\n",
    "    return hits / token_count\n",
    "\n",
    "\n",
    "MIN_CHARS = 400\n",
    "MIN_FOOD_DENSITY = 0.003\n",
    "\n",
    "corpus = []\n",
    "seen_urls = set()\n",
    "crawl_log = {\n",
    "    'config': {\n",
    "        'seeds': SEED_PAGES,\n",
    "        'min_chars': MIN_CHARS,\n",
    "        'min_food_density': MIN_FOOD_DENSITY,\n",
    "    },\n",
    "    'discovery': discovery_log,\n",
    "    'accepted': [],\n",
    "    'skipped': [],\n",
    "    'errors': [],\n",
    "}\n",
    "\n",
    "food_pages = sorted(all_candidate_pages)\n",
    "print(f'Scraping {len(food_pages)} candidate pages...\\n')\n",
    "\n",
    "for i, page_title in enumerate(food_pages, start=1):\n",
    "    try:\n",
    "        result = scrape_wikipedia_page(page_title)\n",
    "        if not result:\n",
    "            crawl_log['skipped'].append({'title': page_title, 'reason': 'page_not_found'})\n",
    "            print(f'[{i}/{len(food_pages)}] - Skipped: {page_title} (not found)')\n",
    "            continue\n",
    "\n",
    "        density = content_food_density(result['text'])\n",
    "        result['food_density'] = round(density, 6)\n",
    "\n",
    "        if result['url'] in seen_urls:\n",
    "            crawl_log['skipped'].append({'title': result['title'], 'reason': 'duplicate_url', 'url': result['url']})\n",
    "            print(f'[{i}/{len(food_pages)}] - Skipped: {result[\"title\"]} (duplicate URL)')\n",
    "        elif result['num_characters'] < MIN_CHARS:\n",
    "            crawl_log['skipped'].append({'title': result['title'], 'reason': 'too_short', 'num_characters': result['num_characters']})\n",
    "            print(f'[{i}/{len(food_pages)}] - Skipped: {result[\"title\"]} (too short)')\n",
    "        elif density < MIN_FOOD_DENSITY:\n",
    "            crawl_log['skipped'].append({'title': result['title'], 'reason': 'low_food_density', 'food_density': round(density, 6)})\n",
    "            print(f'[{i}/{len(food_pages)}] - Skipped: {result[\"title\"]} (low food density {density:.4f})')\n",
    "        else:\n",
    "            seen_urls.add(result['url'])\n",
    "            corpus.append(result)\n",
    "            crawl_log['accepted'].append({'title': result['title'], 'url': result['url'], 'num_characters': result['num_characters']})\n",
    "            print(f'[{i}/{len(food_pages)}] + Accepted: {result[\"title\"]} ({result[\"num_characters\"]} chars)')\n",
    "\n",
    "    except Exception as e:\n",
    "        crawl_log['errors'].append({'title': page_title, 'error': str(e)})\n",
    "        print(f'[{i}/{len(food_pages)}] ! Error: {page_title} -> {e}')\n",
    "\n",
    "    time.sleep(0.3)\n",
    "\n",
    "print(f'\\nAccepted pages: {len(corpus)}')\n",
    "print(f'Skipped pages : {len(crawl_log[\"skipped\"])}')\n",
    "print(f'Errors        : {len(crawl_log[\"errors\"])}')\n"
   ],
   "id": "filter_code"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Scrape Each Page\n",
    "Loop through all discovered food pages and scrape their full text.\n",
    "Text is formatted with markdown headers for use in chunking."
   ],
   "id": "scrape_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "corpus_path = 'data/raw/south_asian_corpus.json'\n",
    "log_path = 'data/raw/crawl_log.json'\n",
    "\n",
    "with open(corpus_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(corpus, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(log_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(crawl_log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f'Corpus saved to: {corpus_path}')\n",
    "print(f'Crawl log saved to: {log_path}')\n",
    "print(f'Total pages: {len(corpus)}')\n",
    "print(f'Total characters: {sum(p[\"num_characters\"] for p in corpus):,}')\n"
   ],
   "id": "scrape_code"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Save Corpus to JSON\n",
    "Save all scraped pages to `data/raw/south_asian_corpus.json`.\n",
    "This file will be loaded by `chunking.ipynb`."
   ],
   "id": "save_md"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('=== CORPUS SUMMARY ===')\n",
    "if not corpus:\n",
    "    print('No pages accepted. Consider lowering thresholds.')\n",
    "else:\n",
    "    total_chars = sum(p['num_characters'] for p in corpus)\n",
    "    print(f'Total pages accepted : {len(corpus)}')\n",
    "    print(f'Total characters     : {total_chars:,}')\n",
    "    print(f'Avg page size        : {total_chars // len(corpus):,} chars')\n",
    "    print(f'Largest page         : {max(corpus, key=lambda x: x[\"num_characters\"])[\"title\"]}')\n",
    "    print(f'Smallest page        : {min(corpus, key=lambda x: x[\"num_characters\"])[\"title\"]}')\n",
    "\n",
    "    print('\\n=== SAMPLE PAGE TITLES ===')\n",
    "    for page in corpus[:10]:\n",
    "        print(f'  - {page[\"title\"]} ({page[\"num_characters\"]:,} chars, density={page.get(\"food_density\", 0):.4f})')\n",
    "\n",
    "    print('\\n=== SAMPLE TEXT PREVIEW (first page) ===')\n",
    "    print(corpus[0]['text'][:500])\n",
    "\n",
    "print('\\n=== CRAWL LOG SUMMARY ===')\n",
    "print(f\"Accepted entries : {len(crawl_log['accepted'])}\")\n",
    "print(f\"Skipped entries  : {len(crawl_log['skipped'])}\")\n",
    "print(f\"Error entries    : {len(crawl_log['errors'])}\")\n"
   ],
   "id": "save_code"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Preview the Corpus\n",
    "Quick look at what was scraped."
   ],
   "id": "preview_md"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Next Step\n",
    "The corpus is now saved at `data/raw/south_asian_corpus.json`.\n",
    "\n",
    "Open `chunking.ipynb` and load this file to start chunking:\n",
    "```python\n",
    "with open('data/raw/south_asian_corpus.json', 'r') as f:\n",
    "    corpus = json.load(f)\n",
    "```"
   ],
   "id": "next_md"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
